Tensorkontraktionen spielen eine zentrale Rolle in vielen wissenschaftlichen Berechnungen und Anwendungen des maschinellen Lernens. Allerdings sind Tensoroperationen oft speicherintensiv und rechenaufwändig und werden dadurch schnell zu einem Bottleneck in Berechnungen. 
Unterschiedliche Lösungsansätze bringen dabei unterschiedliche Vor- und Nachteile mit sich: Während einige Methoden besonders gut für die Berechnung paarweiser Kontraktionen oder kleiner Tensornetzwerkkontraktionen geeignet sind, sind andere wiederum performant auf großen Tensorkontraktionen.
Viele bisherige Implementierungen sind zudem auf bestimmte Datentypen und Tensorstrukturen beschränkt oder können nur eine begrenzte Anzahl von Tensoren verarbeiten, wodurch die Berechnung von Ausdrücken mit vielen Tensoren problematisch wird.\\

\noindent In diesem Projekt stellen wir eine relativ einfache Implementierung zur Berechnung von Tensor-Kontraktionen vor, die auch große und komplexe Tensorkontraktionen effizient berechnen kann. Sie basiert auf dem Transpose-Transpose-BMM-Transpose-Ansatz (TTBT) und nutzt die moderne Einsteinsche Summenkonvention zur Formulierung der zu berechnenden Kontraktion.
Ein Vergleich mit den weit verbreiteten Bibliotheken NumPy~\cite{Numpy} und PyTorch~\cite{PyTorch}, die ebenfalls die Einsteinsche Summenkonvention unterstützen, zeigt, dass unser Ansatz nicht nur konkurrenzfähig ist, sondern in rechenintensiven Szenarien sogar deutlich besser als NumPy abschneidet. Unsere Ergebnisse belegen, dass es möglich ist, eine eigene Tensor-Kontraktions-Engine zu entwickeln, die flexibel, effizient und leistungsstark ist und dabei mit etablierten Bibliotheken mithalten kann.
\section{Calculating Tensor Contractions}
\textcite{springer} state that even though the concept of tensor contractions is quite similar to matrix multiplications, the calculation of these contractions is usually inefficient compared to highly optimized algorithms for matrix multiplications. However, when performing tensor contractions, there are several methods to improve the performance, depending on the characteristics of the tensor and the hardware architecture. \textcite{springer} describe three different approaches to calculating tensor contractions that are used by known tensor contraction libraries.  

\subsection{Nested Loops}
The first and most straightforward approach mentioned by \textcite{springer} involves nested loops, where the indices of the tensors are iterated over. Each loop corresponds to an index of the tensor, and the contraction is achieved by summing over the repeated indices. However, this approach can suffer from poor memory access patterns, especially when the tensors are complex and large, which often leads to suboptimal memory access patterns. Some of these inefficiencies can be mitigated by applying vectorization, loop transformations such as loop reordering or loop fusion that optimize cache efficiency.\\
Numpy ~\cite{Numpy} implements their einsum summation in loops by lowering the expression into a very general form and using a Numpy native iterator loop that effectively becomes a loop-based kernel.

\subsection{Loops-over-GEMMs}
The Loops-over-GEMMs method slices a tensor into multiple 2D sub-tensors (matrices) and contracts them using General Matrix Multiplication (GEMM) operations. This approach is particularly effective when the sub-tensors are large, as it allows efficient use of optimized GEMM routines. However, in some cases, the 2D slices may be small or require strided memory access, which can degrade performance ~\cite{springer}.

\subsection{Transpose-Transpose-GEMM-Transpose (TTGT)}
Another method that is explained by \textcite{springer} is to transpose and flatten the tensors so that they can be interpreted as matrices. This approach involves transposing and flattening the tensors, multiplying them using a General Matrix Multiply (GEMM) operation, and then transposing the result back to match the output tensor's shape. 
One key insight is that every tensor contraction can be represented as a matrix multiplication by dividing the index sets of the two to-be-contracted tensors $A$ and $B$ and the output tensor $C$ into sets $I_A, I_B$, the indices that appear in both $A$ and $C$ or in $B$ and $C$, and the set $I_{con}$, the contracted indices that appear in both $A$ and $B$ but not in $C$. $A$ is then flattened into a matrix $A' \in \mathbb{R}^{I_A\times I_{con}}$, $B$ into a matrix $B' \in \mathbb{R}^{I_{con} \times I_B}$, the result $A'\times B' = C' \in \mathbb{R}^{I_A\times I_B}$ must then be reshaped and transposed, so its indices match the ones given for $C$.
The advantage of this approach is that it reduces the contraction operation to a matrix multiplication, which is a highly optimized operation in most linear algebra libraries such as BLAS ~\cite{BLAS}. However, the need to transpose the tensors and the result introduces an overhead due to additional storage and data movement. The performance of this method is bandwidth-bound if the transposing steps take most of the runtime. In compute-bound scenarios, where the computation is more expensive than the memory transfer, this method can perform very well because GEMM can be highly optimized, especially when using modern hardware, such as GPUs and specialized matrix-multiplication units.

\subsection{GEMM-like Tensor Tensor multiplication (GETT)}\label{gemm}
\noindent \textcite{springer} introduce an approach called ``GEMM-like Tensor Tensor multiplication'' (GETT), that aims to combine the benefits of the aforementioned approaches while avoiding their drawbacks. 
While the concept of GETT is basically similar to that of TTGT, it avoids the overhead of the tensor transposition by implicitly reorganizing the data while packing it into the caches. In the next step, the data that is loaded into the cache can be processed by highly optimized macro-kernels.

\subsection{Transpose-Transpose-BMM-Transpose (TTBT)}
\sloppy
Just as any tensor can be represented as a single matrix, it can also be written in the form of a batched matrix.
By adopting the concept of Transpose–Transpose–GEMM–Transpose, one can replace the GEMM with a batched matrix multiplication (BMM). Similar to the TTGT approach, TTBT usually performs well for compute-bound problems, while the overhead during the pre- and post-processing of the tensors remains a drawback for bandwith-bound calculations.\\
PyTorch’s einsum implementation ~\cite{PyTorch} adopts the TTBT approach by partitioning the index set into subsets and mapping the problem to a BMM whenever its structure allows.

\subsection{Code Generation for Specific Problems}
Another approach is to generate optimized code for specific tensor contractions. TACO (the Tensor Algebra Compiler) ~\cite{kjolstad2017taco} can generate efficient solutions for both sparse and dense tensor operations. However, since the generated code is tailored to a specific pairwise contraction of two given tensors, TACO lacks the flexibility needed to process arbitrary tensor operations dynamically at runtime.

\section{Contraction Paths}
Since tensor contractions are associative, a multi-tensor contraction can be broken down into a sequence of pairwise tensor contractions. The order of these pairwise contractions, called the ``contraction path'', influences the calculation's performance immensely. Consequently, it is crucial to choose an efficient contraction path, but the computation of these paths is very costly itself. 
Optimized Einsum (opt\_einsum) ~\cite{opteinsum} 
is a package for optimizing the tensor contraction order for a problem. For executing pairwise contractions, the user can select from different backends. \textcite{cgreedy} improve the contraction path determination of opt\_einsum and introduce a greedy approach that allows calculating a path either optimized for the size of the biggest intermediate tensor or the total number of floating-point operations needed for the contraction. \textcite{blacher2024einsum} state that choosing the path that optimizes the number of flops does not necessarily result in better performance, since a size-optimized path results in more balanced intermediate tensors and therefore a better cache usage. 

Tensor contractions play an important role in many scientific and machine learning applications. However, due to their complexity and large memory requirements, they can quickly become a performance bottleneck. 
As a result, developing efficient solutions for this class of problems is of great interest. \\
Although various approaches to computing tensor contractions exist, each comes with its own limitations. While some implementations are efficient only for small tensors and perform poorly for compute-bound problems, others are restricted to simple tensor networks structures or specific data types. In this project, our goal is to implement a promising approach to tensor contractions that can handle a wide range of tensor structures and data types, and to compare it to existing engines.\\

\noindent Tensor contractions can be expressed elegantly using the so-called modern Einstein summation notation. The original notation was introduced by Albert Einstein in 1916, from which its modern form is derived ~\cite{einstein}. While brief and concise, it allows for the clear representation of various tensor operations, including element-wise multiplications, dot products, outer products, and matrix multiplications. Since these expressions can be combined, even complex tensor networks can be described in a simple and readable form using Einstein summation. As a result, well-established libraries such as NumPy ~\cite{Numpy} and PyTorch ~\cite{PyTorch} have integrated this notation into their tensor contraction engines. In this project, we adopt the Einstein summation convention to compute tensor contractions. Our goal is to support the contractions that can be expressed with this convention, allowing us to handle complicated tensor structures.\\

\noindent We then compare our implementation to two different established libraries Numpy ~\cite{Numpy} and PyTorch ~\cite{PyTorch}, both of which included Einstein summation into their tensor contraction engines. PyTorch broadly follows the same approach as our implementation, mapping the tensor contraction to a batch matrix multiplication, but relies among others on highly optimized libraries for linear algebra operations for that operation. Numpy uses a different strategy and iterates over the indices of the problem. We find that our implementation is able to compete with both of these engines, being more efficient than Numpy for compute-intensive problems.
 The code to our implementation can be found on \url{https://github.com/fenjaWagner/TTBMMT}.\\
 
 \noindent This paper is structured as follows: We first provide the necessary background information in Section \ref{background} and explain tensors, Einstein summation notation and tensor contractions. In Section \ref{related} we explore different approaches to tensor contractions as well as contraction paths and existing implementations. Then we will describe our algorithm in detail in Section \ref{algorithm}, presenting the benchmark results in Section \ref{experiments}. Finally, we will discuss these results in Section \ref{discussion} and point out possible improvements to our solution.

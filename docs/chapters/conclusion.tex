We have successfully implemented the TTBT approach for tensor contractions expressed in Einstein summation notation by mapping the preprocessed tensors to a relatively simple batch matrix multiplication. We have compared our engine with Numpy's \cite{Numpy} and PyTorch's \cite{PyTorch} einsum library on generated and real-world problems and found that our implementation can compete with these well-known libraries. Moreover, our implementation can handle a wider range of input strings than PyTorch and is more efficient for certain data types. In doing so, we proved it possible to write a simple but yet powerful engine for efficient pairwise tensor contractions that can easily be used for multi-tensor contractions given a contraction path. This engine could be further improved by enhancing memory access patterns of the preprocessing and the BMM implementation.

We have successfully implemented the TTBT approach for tensor contractions expressed in Einstein summation notation by mapping the preprocessed tensors to a relatively simple batch matrix multiplication. We have compared our engine with Numpy's \cite{Numpy} and PyTorch's \cite{PyTorch} einsum library and found, based on generated and real-world problems, that our implementation can compete with these well-known libraries. Moreover, our implementation can handle a wider range of input strings than PyTorch and is even more efficient for certain data types. In doing so, we have demonstrated that it is possible to write a simple yet powerful engine for efficient pairwise tensor contractions that can be easily used for multi-tensor contractions given a contraction path. In doing so, we proved it possible to write a simple but yet powerful engine for efficient pairwise tensor contractions that can be easily used for multi-tensor contractions given a contraction path. This engine can be improved by enhancing memory access patterns of the preprocessing and the BMM implementation.

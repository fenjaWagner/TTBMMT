Tensor contractions are a fundamental operation in many scientific computing and machine learning applications, but their complexity and large memory demands often make them a performance bottleneck.
Different approaches to computing tensor contractions come with different trade-offs â€” depending on the design of the algorithm, certain methods are better suited for small or simple pairwise contractions, while others can handle only complex tensor expressions more efficiently.
Additionaly, many existing implementations are restricted to specific data types and tensor expressions or  can only process a small number of tensors, limiting the execution of multi-tensor contractions versatility.\\
In this project, we present a relatively simple custom implementation for computing tensor contractions, based on the Tensor-Transpose-Batch-GEMM-Transpose (TTBT) approach and expressed using the concise and powerful modern Einstein summation notation. Despite its simplicity, our algorithm is capable of handling arbitrarily large and complex tensor expressions.\\
The comparison of our implementation against two widely used libraries, Numpy~\cite{Numpy} and PyTorch~\cite{PyTorch}, both of which support Einstein summation, shows that our implementation can compete with both libraries, outperforming NumPy in compute-intensive scenarios. These findings show that it is possible to build a custom tensor contraction engine that stays flexible and efficient, handling complex tensor expressions while still keeping up with well-established libraries.
